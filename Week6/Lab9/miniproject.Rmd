---
title: 'Lab 9: Mini Project'
author: "Vivian Cai"
date: "2/12/2022"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

## 1. Explorotory Data Analysis  

Before starting the project, I downloaded the WisconsinCancer.csv file from the class website and moved it into the Lab9 folder (same directory as my R markdown file)  

```{r}
# Save  input data file into Project directory
fna.data <- "WisconsinCancer.csv"

# input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)
head(wisc.df)
dim(wisc.df)
```
```{r}
# Creating a new data frame what omits the diagnosis column
wisc.data <- wisc.df[,-1]
head(wisc.data)
```

```{r}
# also create a factor using only the diagnosis column
diagnosis <- factor(wisc.df[,1])
head(diagnosis)
```

#### Q1. How many observations are in this dataset?  

```{r}
dim(wisc.data)
```
The data frame has **569** observations.  

#### Q2. How many of the observations have a malignant diagnosis?  

```{r}
sum(diagnosis == "M")
```
Out of the 569 observations, **212** are malignant.  

#### Q3. How many variables/features in the data are suffixed with _mean?  

```{r}
# save column names as a new vector
wisc.colnames <- c(colnames(wisc.df))
wisc.colnames

# find the number of elements in wisc.colnames that contains _mean
length(grep("_mean", wisc.colnames))
```
**10** features from the data are suffixed with "_mean".  

## 2. Principle Component Analysis  

```{r}
# Check column means and standard deviations
colMeans(wisc.data)
apply(wisc.data,2,sd)
```
```{r}
# Perform PCA on wisc.data, since the data are on different magnitudes upon first inspection, we set scale. to TRUE
wisc.pr <- prcomp( wisc.data, center = TRUE, scale. = TRUE )
summary(wisc.pr)
```
#### Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?  

**44.27%**  

#### Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?  

**3** (cumulative proportion achieves 72.6% at PC3) 

#### Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?  

**7** (cumulative proportion achieves 91% at PC7)  

```{r}
biplot(wisc.pr)
```

#### Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?  

The plot is very difficult to understand because everything is on top of each other and we can't really see what is going on.(30PCs are too many!)  

```{r}
# Scatter plot observations by components 1 and 2
plot( wisc.pr$x[,1], wisc.pr$x[,2] , col = diagnosis , 
     xlab = "PC1", ylab = "PC2")
```

#### Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?  
We see an overall **separation of "M"(red) from "B"(black) samples**. **The plot of PC1 vs. PC2 has better separation on the y-axis than that of the PC1 vs. PC3 plot**. This is reasonable because PC2 captures more variance in the data than PC3.  

```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[,1], wisc.pr$x[,3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```
```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col= diagnosis) + 
  geom_point()
```
```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```
```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```
```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```
```{r}
## ggplot based graph
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

#### Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?  

```{r}
wisc.pr$rotation["concave.points_mean",1]
```
The loading vector of concave.points_mean for PC1 is about **-0.26**.  

#### Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?  

Based on the scree plots and the variance table, **5 PCs** are required to explain 80% of the data.  

## 3. Hierarchical Clustering  

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```

```{r}
# Calculate the (Euclidean) distances between all
# pairs of observations in the new scaled dataset 
# and assign the result to data.dist
data.dist <- dist(data.scaled)
```

```{r}
# Create a hierarchical clustering model using 
# complete linkage. Manually specify the method
# argument to hclust() and assign the results to
# wisc.hclust
wisc.hclust.complete <- hclust(data.dist, method = "complete")
```

#### Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?  

As shown, at height **19**, the model has 4 cluster.  

```{r}
plot(wisc.hclust.complete)
abline(h = 15, col="blue", lty=2)
abline(h = 17, col="green", lty=2)
abline(h = 19, col="red", lty=2)
abline(h = 20, col="purple", lty=2)
```
```{r}
wisc.hclust.clusters.complete <- cutree(wisc.hclust.complete, k = 4)
table(wisc.hclust.clusters.complete, diagnosis)
```

#### Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?  

```{r}
wisc.hclust.clusters.complete9 <- cutree(wisc.hclust.complete, k = 9)
table(wisc.hclust.clusters.complete9, diagnosis)
```
Using **k = 9**, the result is slightly better.  

#### Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.  

Side-note: The method="ward.D2"creates groups such that variance is minimized within clusters. This has the effect of looking for spherical clusters with the process starting with all points in individual clusters (bottom up) and then repeatedly merging a pair of clusters such that when merged there is a minimum increase in total within-cluster variance This process continues until a single group including all points (the top of the tree) is defined.  

```{r}
wisc.hclust <- hclust(data.dist, method = "centroid")
plot(wisc.hclust)
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
table(wisc.hclust.clusters, diagnosis)
```
```{r}
wisc.hclust <- hclust(data.dist, method = "median")
plot(wisc.hclust)
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
table(wisc.hclust.clusters, diagnosis)
```
```{r}
wisc.hclust <- hclust(data.dist, method = "ward.D2")
plot(wisc.hclust)
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
table(wisc.hclust.clusters, diagnosis)
```
I like the **"ward.D2"** method the best for it gives the neatest graph.The bottom-up grouping is also suitable for this dataset for we are eventually looking at either benign/malignant samples.However, the "complete" method seems to give the cleanest separation of the two sample types.  

## 4. K-means Clustering  

```{r}
# Create a k-means model on wisc.data, assigning the result to wisc.km.
wisc.km <- kmeans(scale(wisc.data), centers= 2, nstart= 20)

# Use the table() function to compare the cluster membership of the k-means model
table(wisc.km$cluster, diagnosis)

table(wisc.hclust.clusters.complete, wisc.km$cluster)
```

#### Q14. How well does k-means separate the two diagnoses? How does it compare to your hclust results?  

k-means seem to have done a better job than hclust by giving two clusters with very good separation based on diagnosis.  

## 5. Combining Methods  

*Using the minimum number of principal components required to describe at least 90% of the variability in the data, create a hierarchical clustering model with the linkage method="ward.D2". We use Ward’s criterion here because it is based on multidimensional variance like principal components analysis. Assign the results to wisc.pr.hclust.*  

```{r}
# Using the minimum number of principal components
# required to describe at least 90% of the
# variability in the data (PC1-7), create a
# hierarchical clustering model with the linkage
# method="ward.D2". We use Ward’s criterion here
# because it is based on multidimensional variance
# like principal components analysis. Assign the
# results to wisc.pr.hclust.

wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method = "ward.D2")
plot(wisc.pr.hclust)
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```
```{r}
table(grps, diagnosis)
```
```{r}
plot(wisc.pr$x[,1:2], col=grps)
plot(wisc.pr$x[,1:2], col=diagnosis)

# Note the color swap here as the hclust cluster 1 
# is mostly “M” and cluster 2 is mostly “B” as we 
# saw from the results of calling table(grps,
# diagnosis). To match things up we can turn our
# groups into a factor and reorder the levels so
# cluster 2 comes first and thus gets the first 
# color (black) and cluster 1 gets the second color
# (red).
g <- as.factor(grps)
levels(g)
```
```{r}
g <- relevel(g,2)
levels(g)
```
```{r}
# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
```
```{r}
# Fancy 3D plot
library(rgl)
plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", cex=1.5, size=1, type="s", col=grps)
rglwidget(width = 400, height = 400)
```
```{r}
# Use the distance along the first 7 PCs for
# clustering i.e. wisc.pr$x[, 1:7]
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method="ward.D2")

# Cut this hierarchical clustering model into 2
# clusters and assign the results to
# wisc.pr.hclust.clusters.
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)

# Compare to actual diagnoses
table(wisc.pr.hclust.clusters, diagnosis)
```

#### Q15. How well does the newly created model with two clusters separate out the two diagnoses?  

It did a better job than before and is pretty similar to the k-means results.  

#### Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.  

```{r}
table(wisc.km$cluster, diagnosis)
table(wisc.hclust.clusters.complete, diagnosis)
```
Before PCA, the clustering is a lot messier and requires more than two clusters to achieve decent separation of the 2 diagnosis. After PCA, the separation is much cleaner.  

## 6. Sensitivity/Specificity  

#### Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?  

**Sensitivity** refers to a test’s ability to correctly detect ill patients who do have the condition. In our example here the sensitivity is the total number of samples in the cluster identified as predominantly malignant (cancerous) divided by the total number of known malignant samples. In other words: **TP/(TP+FN)**.  

```{r}
TPFN <- sum(diagnosis == "M")
kmeans.sensi = 175/TPFN
prWard2.sensi = 188/TPFN
```
  
**Specificity** relates to a test’s ability to correctly reject healthy patients without a condition. In our example specificity is the proportion of benign (not cancerous) samples in the cluster identified as predominantly benign that are known to be benign. In other words: **TN/(TN+FN)**.  

```{r}
TNFN <- sum(diagnosis == "B")
kmeans.speci = 343/TNFN
prWard2.speci = 329/TNFN
```
Based on the above calculations, the kmeans clustering has higher specificity while the h-clust after PCA method has higher sensitivity.  

## 7. Prediction  

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```
```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

#### Q18. Which of these new patients should we prioritize for follow up based on your results?  

Based on our clustering, patent 2 is located closer to the center of the maliganent cluster, therefore, we should give priority to **patient 2**.